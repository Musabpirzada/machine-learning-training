{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0cYLnQGNsjEo"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get a hardware device for training"
      ],
      "metadata": {
        "id": "eLW1SU1Fs5F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoFe79tes6qk",
        "outputId": "606a1993-436c-41db-c148-4bd6933f7a55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "lN60r405s9ZG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh7rBXhGs_BU",
        "outputId": "ef7e83cd-92a5-49d4-fc01-5b676ca28e5d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X) \n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI1w3er2tAoL",
        "outputId": "3e9da874-d398-4103-8000-dd579b6f389c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight and Bias"
      ],
      "metadata": {
        "id": "6LKrFkMotBxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First Linear weights: {model.linear_relu_stack[0].weight} \\n\")\n",
        "\n",
        "print(f\"First Linear weights: {model.linear_relu_stack[0].bias} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfyv-DQBtDmw",
        "outputId": "ee2dd604-f24d-4b43-e48a-2aa5e176b225"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Linear weights: Parameter containing:\n",
            "tensor([[ 0.0039,  0.0103,  0.0046,  ...,  0.0167,  0.0215,  0.0308],\n",
            "        [-0.0276, -0.0319, -0.0336,  ...,  0.0349, -0.0216,  0.0129],\n",
            "        [-0.0131,  0.0074,  0.0215,  ..., -0.0154, -0.0330, -0.0107],\n",
            "        ...,\n",
            "        [ 0.0276, -0.0316, -0.0107,  ..., -0.0130, -0.0072, -0.0187],\n",
            "        [-0.0018,  0.0277,  0.0138,  ..., -0.0182,  0.0011, -0.0179],\n",
            "        [-0.0201,  0.0144, -0.0110,  ..., -0.0014, -0.0252, -0.0316]],\n",
            "       requires_grad=True) \n",
            "\n",
            "First Linear weights: Parameter containing:\n",
            "tensor([ 2.8117e-02, -3.2134e-02,  2.5106e-02,  2.0278e-02, -9.6691e-03,\n",
            "        -1.4740e-02, -1.7831e-03, -2.1756e-02,  1.1263e-02, -1.1066e-02,\n",
            "         9.3302e-03,  2.6823e-02,  3.1190e-02, -9.2579e-04,  3.4672e-02,\n",
            "         2.4640e-02, -1.6723e-02,  2.0469e-02, -3.0628e-02, -4.8623e-03,\n",
            "         3.0212e-02, -3.2355e-02,  1.1663e-02,  3.4431e-03, -2.5192e-02,\n",
            "        -5.9952e-03, -1.2758e-02, -3.1498e-02, -2.0380e-02, -1.5245e-02,\n",
            "         2.0806e-02,  1.2943e-02, -3.0288e-02, -1.8293e-02,  2.8864e-02,\n",
            "         2.8845e-02,  1.8358e-02, -3.3852e-02,  1.8302e-02, -1.3418e-03,\n",
            "         1.9370e-02,  1.7764e-02, -2.9544e-02,  1.1256e-02,  8.3844e-03,\n",
            "        -2.8768e-02,  1.6435e-02, -2.7014e-02,  1.4900e-02, -3.2995e-02,\n",
            "         4.9221e-04,  5.7053e-03, -1.5469e-02,  2.5395e-02, -3.5641e-02,\n",
            "         2.5554e-02,  9.5592e-04,  2.5720e-02,  3.4481e-02,  2.8245e-02,\n",
            "        -6.0360e-03, -3.0938e-02, -1.5875e-03,  8.7440e-03, -2.2949e-02,\n",
            "         5.3467e-03,  1.2117e-02, -3.1712e-02,  3.2450e-02,  2.7495e-02,\n",
            "        -3.1932e-02,  3.4828e-02, -2.9496e-02, -4.8601e-03,  3.2307e-02,\n",
            "        -7.5463e-04,  2.6430e-02, -2.4131e-02, -1.9975e-02,  2.4984e-02,\n",
            "         2.6012e-02, -2.2591e-02,  2.1017e-02, -1.1300e-02, -9.0063e-03,\n",
            "        -1.3366e-02,  1.3659e-02, -4.0781e-03, -9.4787e-03,  1.9345e-02,\n",
            "        -7.1608e-03,  1.3968e-02,  1.0056e-02,  2.4186e-02, -3.3905e-02,\n",
            "        -4.9219e-04, -7.7374e-03,  1.4908e-02,  6.3005e-03,  2.8484e-02,\n",
            "         1.5679e-03, -2.5803e-02,  1.8591e-02,  3.0101e-02, -3.0992e-02,\n",
            "         3.4903e-02, -2.0526e-02, -2.4682e-02,  9.0681e-03,  2.9366e-02,\n",
            "         1.0278e-02, -2.8829e-02,  2.0301e-02, -2.1745e-02, -1.1404e-02,\n",
            "        -1.8066e-02, -7.2881e-03, -1.2367e-02,  2.6989e-02, -8.6511e-04,\n",
            "         1.3518e-02,  3.3544e-02,  2.0729e-02, -1.5602e-02,  3.3933e-02,\n",
            "        -1.0800e-02, -2.7295e-02, -1.3908e-02, -1.5485e-02,  2.6749e-03,\n",
            "        -1.7435e-02,  1.2032e-02, -1.8676e-02,  1.3546e-02,  3.4532e-03,\n",
            "         2.5974e-02,  2.5703e-02,  1.9540e-02,  2.8770e-02, -2.3452e-02,\n",
            "        -7.7829e-03,  2.1148e-02,  4.3890e-03, -2.6812e-02, -3.4597e-02,\n",
            "        -6.4799e-03, -3.1578e-02, -6.6120e-03,  1.9266e-02,  2.4438e-02,\n",
            "         7.6807e-03,  2.8279e-02, -7.0581e-03,  2.2319e-02,  9.5272e-03,\n",
            "        -2.4722e-02, -2.5174e-05,  9.9884e-03, -2.7194e-02, -3.4155e-02,\n",
            "        -5.9016e-03, -1.7834e-02, -2.0170e-02, -2.2182e-02,  2.6360e-03,\n",
            "         3.3976e-02,  2.4103e-02, -2.1636e-02, -3.0194e-02, -2.8640e-02,\n",
            "        -3.2559e-02,  3.6198e-03,  2.4122e-02,  3.0576e-02,  1.7923e-02,\n",
            "        -2.5637e-02, -4.5620e-03,  2.0453e-02, -1.9184e-02, -1.9957e-02,\n",
            "        -3.4623e-02, -1.6976e-02,  1.1432e-02, -1.3267e-02, -2.2396e-02,\n",
            "         5.1525e-03, -1.1152e-02, -3.0336e-02,  2.0081e-03,  1.4600e-02,\n",
            "         3.0346e-02,  1.6360e-02, -2.4341e-02, -3.0218e-02, -7.1308e-03,\n",
            "         7.5503e-03, -4.3614e-03, -2.7538e-03,  7.3521e-03, -3.4188e-02,\n",
            "         2.7968e-02, -7.6836e-03, -2.4562e-02, -2.6435e-02,  2.5195e-02,\n",
            "         6.1336e-03,  1.6390e-02, -2.7930e-02, -2.4497e-02,  3.4174e-02,\n",
            "        -3.0037e-02, -2.1906e-02,  2.5110e-02,  3.4319e-02, -1.6729e-02,\n",
            "        -2.3331e-02,  2.0805e-03, -2.4348e-02,  1.6564e-02,  6.3366e-03,\n",
            "         2.4365e-02, -3.2805e-02, -5.2517e-03, -6.0433e-03,  6.0524e-03,\n",
            "         1.0709e-02, -6.1720e-03,  2.2076e-02, -5.6173e-03,  8.5277e-05,\n",
            "        -3.4710e-02, -4.2548e-03,  3.8157e-04, -2.9664e-02,  3.0959e-02,\n",
            "        -4.8955e-03,  2.5536e-02, -1.6477e-02, -1.6534e-02, -3.3101e-02,\n",
            "        -2.6259e-03,  2.8188e-02, -2.8198e-02, -3.3998e-03, -1.9610e-02,\n",
            "        -1.5073e-02, -3.4820e-02, -3.0911e-02,  1.8692e-02, -2.7252e-02,\n",
            "        -2.3178e-03, -1.0855e-02, -3.2850e-02,  1.1795e-02, -6.7016e-03,\n",
            "        -2.1884e-02,  6.7667e-03, -3.5008e-02, -9.6757e-03, -3.2157e-02,\n",
            "         1.7532e-02, -2.3599e-03,  1.6123e-02, -1.4684e-03,  1.3510e-02,\n",
            "         5.5339e-03, -1.9085e-02,  2.0120e-02,  3.2862e-02, -8.7087e-03,\n",
            "         4.6035e-03,  9.4093e-03, -3.1623e-02, -7.6410e-03,  1.4030e-02,\n",
            "        -2.2639e-02, -2.9255e-02, -3.5476e-02, -1.2973e-02,  9.2732e-03,\n",
            "         3.4446e-02, -6.0881e-03, -2.2542e-02, -2.1997e-02, -3.6540e-03,\n",
            "        -1.2083e-02,  6.9261e-05,  3.5588e-05, -3.0066e-02, -3.4524e-02,\n",
            "        -2.7544e-02, -2.3874e-02, -9.3357e-03, -1.8740e-02,  1.4830e-02,\n",
            "         3.3349e-02,  1.0483e-02, -2.9506e-02,  1.3842e-04,  2.7861e-02,\n",
            "         2.1180e-02, -1.7231e-02,  1.5849e-03, -2.5610e-02,  1.6865e-03,\n",
            "         4.2284e-03, -1.2277e-02,  6.0578e-03,  7.1012e-03,  2.9451e-03,\n",
            "         8.7086e-03,  2.9834e-02, -1.9904e-02, -1.3847e-02,  3.2124e-02,\n",
            "        -2.4639e-02, -2.4500e-04, -3.3790e-02,  2.2891e-02,  1.1228e-02,\n",
            "         3.0513e-02,  3.4485e-02,  2.1640e-02, -1.3187e-02, -6.3892e-03,\n",
            "        -3.3613e-02,  3.5437e-03,  2.1914e-03, -1.3163e-02, -2.3613e-02,\n",
            "         6.0470e-03, -2.5570e-03, -2.9469e-02, -1.3693e-02,  2.6041e-02,\n",
            "        -2.0048e-02,  3.3680e-02, -1.5549e-03, -1.6619e-02,  3.3470e-02,\n",
            "        -7.3547e-03, -2.0235e-02, -2.4218e-02, -2.6828e-02, -5.7263e-03,\n",
            "        -6.0149e-03,  1.0604e-02, -3.2786e-02,  2.5667e-02,  2.0241e-02,\n",
            "         1.1581e-02, -1.5224e-02, -8.1576e-03, -1.1594e-02,  2.8358e-02,\n",
            "         3.0441e-02,  1.0021e-02,  1.3519e-02,  1.5281e-03, -3.2347e-02,\n",
            "         2.0160e-02,  3.5517e-03,  3.2170e-02, -2.6042e-02, -7.7918e-03,\n",
            "        -2.2082e-02,  3.3097e-02,  1.0094e-02, -1.8012e-02, -3.3330e-04,\n",
            "         4.6799e-03, -1.5043e-02, -3.0578e-02, -1.6888e-02,  2.4690e-02,\n",
            "        -2.4498e-02,  1.7585e-02, -8.5194e-03, -3.1221e-02, -2.1829e-02,\n",
            "        -2.2414e-02, -2.9528e-02, -5.1573e-03,  1.0143e-02, -2.3364e-02,\n",
            "        -1.4316e-03,  1.4089e-02,  8.7438e-03,  1.9336e-03,  2.9432e-02,\n",
            "         1.2941e-02,  7.3533e-03, -4.3293e-03,  1.6254e-03,  7.2435e-04,\n",
            "        -2.5033e-02,  1.9419e-02, -1.5506e-02, -2.6945e-02,  3.1387e-02,\n",
            "         2.8860e-02,  2.2427e-02,  3.1157e-02,  1.9818e-02, -2.3664e-02,\n",
            "         2.9517e-02, -3.2538e-02,  2.0882e-02,  2.6559e-02,  8.1977e-03,\n",
            "         1.9891e-02,  1.5736e-02,  1.4527e-02, -4.1173e-03, -1.8724e-02,\n",
            "        -2.8149e-02,  8.2536e-03, -5.9009e-03,  1.7288e-02,  2.7395e-02,\n",
            "         3.5351e-02,  8.6919e-03,  1.6791e-02,  7.0216e-03, -7.7508e-04,\n",
            "         2.2632e-02, -2.0650e-02,  2.6569e-02,  1.3217e-02, -1.9786e-02,\n",
            "         1.2591e-02, -2.5192e-02,  8.3379e-03, -3.5448e-02,  3.5335e-02,\n",
            "         3.6964e-03, -1.2171e-02, -2.6725e-02, -6.9127e-03,  9.7385e-03,\n",
            "         2.4896e-02, -2.8663e-02,  2.2424e-02, -3.2872e-02, -1.1503e-02,\n",
            "        -2.3485e-02,  8.6429e-03,  1.1319e-02,  1.9340e-02,  2.1721e-02,\n",
            "        -9.5459e-03,  5.9786e-04, -4.1895e-03, -2.9739e-03,  3.1998e-02,\n",
            "        -3.3882e-02,  3.5111e-02,  1.3474e-02,  3.4939e-02, -3.5315e-02,\n",
            "         2.4736e-02, -2.6629e-03, -2.1527e-02, -6.7258e-03,  2.9332e-02,\n",
            "         5.8598e-03, -2.2269e-02,  1.6851e-02,  1.9194e-02,  2.5455e-03,\n",
            "         2.6579e-02,  1.5461e-02, -2.1840e-02, -2.5080e-02,  8.5139e-03,\n",
            "        -1.3540e-02,  9.3618e-03,  3.4193e-02,  2.2027e-02,  3.1377e-02,\n",
            "        -3.1201e-02,  1.2531e-02,  7.4464e-03, -1.7528e-02, -4.7305e-03,\n",
            "         2.2656e-02, -7.6006e-03,  8.5787e-03,  1.3090e-03,  2.7919e-02,\n",
            "        -2.0173e-02, -3.2487e-02, -1.5388e-02,  2.3541e-02, -2.5175e-02,\n",
            "         2.2026e-02,  3.5308e-02,  3.2617e-02,  1.9622e-02, -7.3493e-03,\n",
            "         3.1714e-02,  1.6247e-02,  3.2581e-02, -2.1181e-02, -8.3320e-03,\n",
            "        -1.7675e-02, -2.5997e-02,  2.0654e-02,  5.7096e-03, -1.1825e-02,\n",
            "        -3.3731e-02,  2.7677e-02], requires_grad=True) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model layers"
      ],
      "metadata": {
        "id": "AbVS3CvMtFfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTjWoNXftHgk",
        "outputId": "5611850d-6441-4e22-f0ca-616bb3d33b27"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Flatten"
      ],
      "metadata": {
        "id": "ap_HHqEhtKMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LIYjuuotLi7",
        "outputId": "9d6c1097-5bd0-476d-cf46-f39b70645140"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Linear"
      ],
      "metadata": {
        "id": "BG3bFVjbtNOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_B6HkbuOtOlG",
        "outputId": "f703d5bc-ad48-48d2-c1c2-27f4b11b8eab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.ReLU"
      ],
      "metadata": {
        "id": "5f58-uk1tQQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv5o1zwetRuM",
        "outputId": "857f6334-dd95-4194-cf0b-408e9325d1ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before ReLU: tensor([[-5.8249e-02,  9.7416e-02, -3.3519e-01,  1.0030e-01, -3.2211e-01,\n",
            "          1.1322e-01,  5.2531e-01,  7.8854e-01,  7.0248e-01,  2.3923e-01,\n",
            "         -1.6533e-01, -7.0195e-02,  4.9392e-01, -4.0511e-04, -6.8302e-01,\n",
            "         -3.4399e-01, -5.6296e-01, -5.0802e-01, -2.5211e-01,  5.9499e-02],\n",
            "        [-8.0393e-02, -2.5581e-01, -1.5525e-01, -5.3405e-02, -5.3444e-01,\n",
            "          4.0904e-02,  5.2812e-01,  7.5067e-01,  4.6755e-01,  1.5713e-01,\n",
            "         -3.9585e-01,  2.3661e-01,  6.7650e-01,  1.5113e-01, -2.6090e-01,\n",
            "         -3.8531e-01, -2.8772e-01, -4.1814e-01, -2.8913e-01,  2.3431e-01],\n",
            "        [-6.3723e-02, -2.5305e-01, -3.2939e-01, -2.9992e-01, -6.3004e-01,\n",
            "         -9.6184e-02, -4.2360e-02,  1.0509e+00,  5.7322e-01,  2.1034e-01,\n",
            "         -5.7424e-01,  2.3056e-01,  3.8315e-01,  1.3608e-01, -3.7267e-01,\n",
            "         -5.1911e-01, -5.1301e-01, -1.8171e-01, -3.1197e-01,  2.9290e-01]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.0000, 0.0974, 0.0000, 0.1003, 0.0000, 0.1132, 0.5253, 0.7885, 0.7025,\n",
            "         0.2392, 0.0000, 0.0000, 0.4939, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0595],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0409, 0.5281, 0.7507, 0.4676,\n",
            "         0.1571, 0.0000, 0.2366, 0.6765, 0.1511, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.2343],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0509, 0.5732,\n",
            "         0.2103, 0.0000, 0.2306, 0.3832, 0.1361, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.2929]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Sequential"
      ],
      "metadata": {
        "id": "R5lXuN2ltUPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ],
      "metadata": {
        "id": "Cc3ZQVFttWHm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Softmax"
      ],
      "metadata": {
        "id": "FqNL9wJctYJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ],
      "metadata": {
        "id": "7TBjnnwqtZ0l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model parameters"
      ],
      "metadata": {
        "id": "JnFEp2dstb_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model structure: \", model, \"\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbmosbptdPk",
        "outputId": "f06d6f45-00c3-4a29-84f1-1df9f02096a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure:  NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ") \n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0039,  0.0103,  0.0046,  ...,  0.0167,  0.0215,  0.0308],\n",
            "        [-0.0276, -0.0319, -0.0336,  ...,  0.0349, -0.0216,  0.0129]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0281, -0.0321], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0416, -0.0315,  0.0018,  ...,  0.0046, -0.0214,  0.0400],\n",
            "        [-0.0365,  0.0014,  0.0291,  ...,  0.0024, -0.0294,  0.0440]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0150,  0.0032], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0098, -0.0018, -0.0432,  ..., -0.0415, -0.0084,  0.0142],\n",
            "        [-0.0043, -0.0184, -0.0370,  ..., -0.0018,  0.0351,  0.0273]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0172,  0.0116], grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    }
  ]
}